{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--1th video get, samples : 46 / total samples : 46--\n",
      "--2th video get, samples : 47 / total samples : 93--\n",
      "--3th video get, samples : 57 / total samples : 150--\n",
      "--7th video get, samples : 48 / total samples : 198--\n",
      "--8th video get, samples : 44 / total samples : 242--\n",
      "--9th video get, samples : 35 / total samples : 277--\n",
      "--10th video get, samples : 24 / total samples : 301--\n",
      "--11th video get, samples : 18 / total samples : 319--\n",
      "--12th video get, samples : 35 / total samples : 354--\n",
      "--13th video get, samples : 35 / total samples : 389--\n",
      "--14th video get, samples : 48 / total samples : 437--\n",
      "--15th video get, samples : 48 / total samples : 485--\n",
      "--16th video get, samples : 42 / total samples : 527--\n",
      "--17th video get, samples : 31 / total samples : 558--\n",
      "--20th video get, samples : 37 / total samples : 595--\n",
      "--21th video get, samples : 31 / total samples : 626--\n",
      "--22th video get, samples : 34 / total samples : 660--\n",
      "--23th video get, samples : 15 / total samples : 675--\n",
      "--24th video get, samples : 30 / total samples : 705--\n",
      "--25th video get, samples : 27 / total samples : 732--\n",
      "--26th video get, samples : 5 / total samples : 737--\n",
      "--31th video get, samples : 5 / total samples : 742--\n",
      "--38th video get, samples : 20 / total samples : 762--\n",
      "--47th video get, samples : 42 / total samples : 804--\n",
      "--48th video get, samples : 5 / total samples : 809--\n",
      "--50th video get, samples : 4 / total samples : 813--\n",
      "--53th video get, samples : 10 / total samples : 823--\n",
      "--54th video get, samples : 6 / total samples : 829--\n",
      "--55th video get, samples : 20 / total samples : 849--\n",
      "--57th video get, samples : 40 / total samples : 889--\n",
      "--58th video get, samples : 37 / total samples : 926--\n",
      "--59th video get, samples : 23 / total samples : 949--\n",
      "--60th video get, samples : 64 / total samples : 1013--\n",
      "--61th video get, samples : 46 / total samples : 1059--\n",
      "--62th video get, samples : 127 / total samples : 1186--\n",
      "--63th video get, samples : 126 / total samples : 1312--\n",
      "--64th video get, samples : 120 / total samples : 1432--\n",
      "--66th video get, samples : 36 / total samples : 1468--\n",
      "--67th video get, samples : 47 / total samples : 1515--\n",
      "--68th video get, samples : 15 / total samples : 1530--\n",
      "--69th video get, samples : 47 / total samples : 1577--\n",
      "--70th video get, samples : 15 / total samples : 1592--\n",
      "--71th video get, samples : 288 / total samples : 1880--\n",
      "--72th video get, samples : 503 / total samples : 2383--\n",
      "--73th video get, samples : 355 / total samples : 2738--\n",
      "--74th video get, samples : 565 / total samples : 3303--\n",
      "--76th video get, samples : 16 / total samples : 3319--\n",
      "--78th video get, samples : 6 / total samples : 3325--\n",
      "--79th video get, samples : 126 / total samples : 3451--\n",
      "--81th video get, samples : 15 / total samples : 3466--\n",
      "--84th video get, samples : 26 / total samples : 3492--\n",
      "--85th video get, samples : 73 / total samples : 3565--\n",
      "--86th video get, samples : 237 / total samples : 3802--\n",
      "--87th video get, samples : 145 / total samples : 3947--\n",
      "--88th video get, samples : 9 / total samples : 3956--\n",
      "--89th video get, samples : 42 / total samples : 3998--\n",
      "--90th video get, samples : 61 / total samples : 4059--\n",
      "--93th video get, samples : 9 / total samples : 4068--\n",
      "--101th video get, samples : 1 / total samples : 4069--\n",
      "--104th video get, samples : 7 / total samples : 4076--\n",
      "--111th video get, samples : 33 / total samples : 4109--\n",
      "--112th video get, samples : 26 / total samples : 4135--\n",
      "--113th video get, samples : 38 / total samples : 4173--\n",
      "--115th video get, samples : 56 / total samples : 4229--\n",
      "--119th video get, samples : 1 / total samples : 4230--\n",
      "--127th video get, samples : 38 / total samples : 4268--\n",
      "-------------------Dataset load finished----------------------\n",
      "-------------Total : 4268 Samples from 66 Videos----------------\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# multi-step encoder-decoder lstm \n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import models as M\n",
    "\n",
    "size_grid = 24\n",
    "size_sample = 30 #input size + predict output size(1)  (timesteps)\n",
    "#Let's define size_batch to make batches with the fixed size\n",
    "size_batch = 10\n",
    "\n",
    "\n",
    "#Load the pre dataset\n",
    "total_video = []\n",
    "one_video = []  #human grid pixel change data\n",
    "one_case = []\n",
    "point_changes = []  #temporal pixel changes place for one video\n",
    "for i in range(size_grid):  \n",
    "\tpoint_changes.append([])\n",
    "\n",
    "video_index = -1\n",
    "f = open('clap2.txt', mode = 'rt')\n",
    "\n",
    "count_sample = 0\n",
    "current_frame = 0\n",
    "\n",
    "# size_sample frame per one clap(assumption) 15 for input 1 for output\n",
    "while True:\n",
    "\tline = f.readline()\n",
    "    #for the last point_changes, one last for and then break while\n",
    "\tif not line: \n",
    "\t\tif current_frame >= size_sample :\n",
    "\t\t\tprint('--%dth video get, samples : %d / total samples : %d--'%(video_index-1,len(one_video), count_sample))            \n",
    "\t\t\ttotal_video.append(one_video)\n",
    "\t\tbreak        \n",
    "\tnumbers = line.split(' ')  #1 1 : 0 0 0 0 0 0 0 (ex)\n",
    "    \n",
    "\tif video_index != int(numbers[0]) :\n",
    "\t\tvideo_index = int(numbers[0])\n",
    "\t\t#if the previous frame index is same or bigger than size_sample, there's at least one case to one video\n",
    "\t\tif current_frame >= size_sample : \n",
    "\t\t\tprint('--%dth video get, samples : %d / total samples : %d--'%(video_index-1,len(one_video), count_sample))\n",
    "\t\t\ttotal_video.append(one_video)   #add samples to one video sample set\n",
    "\t\tfor k in range(size_grid):\n",
    "\t\t\tpoint_changes[k] = []  #reset (new video has begun)  \n",
    "\t\tone_video = []  #reset (new video has begun)            \n",
    "            \n",
    "    #add new value\n",
    "\tfor i in range(size_grid):      \n",
    "\t\tval_int = int(numbers[i+3])        \n",
    "\t\tpoint_changes[i].append(val_int)\n",
    "        \n",
    "    #add new sample if the number of values is same of bigger than size_sample\n",
    "\tcurrent_frame = int(numbers[1])       \n",
    "\tif current_frame < size_sample :\n",
    "\t\tcontinue\n",
    "\telse:\n",
    "\t\tstart = current_frame - size_sample        \n",
    "\t\t#recent ones for one_case         \n",
    "\t\tfor i in range(size_sample):  # ex) one_case = ((0,0,1,1,0,1), (0,0,2,2,0,2), ... )\n",
    "\t\t\ttemp = []\n",
    "\t\t\tfor k in range(size_grid):   # ex) (0, 0 ,1, 1, 0, 1)\n",
    "\t\t\t\ttemp.append(point_changes[k][start + i])                  \n",
    "\t\t\tone_case.append(temp)   #one case with 20 time stamps\n",
    "            \n",
    "\t\tone_video.append(one_case) #total cases               \n",
    "\t\tone_case = []        \n",
    "\t\tcount_sample += 1         \n",
    "          \n",
    "f.close()\n",
    "print(\"-------------------Dataset load finished----------------------\")\n",
    "print(\"-------------Total : %d Samples from %d Videos----------------\"%(count_sample, len(total_video)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th video\n",
      "4 batches generated\n",
      "2th video\n",
      "4 batches generated\n",
      "3th video\n",
      "5 batches generated\n",
      "4th video\n",
      "4 batches generated\n",
      "5th video\n",
      "4 batches generated\n",
      "6th video\n",
      "3 batches generated\n",
      "7th video\n",
      "2 batches generated\n",
      "8th video\n",
      "1 batches generated\n",
      "9th video\n",
      "3 batches generated\n",
      "10th video\n",
      "3 batches generated\n",
      "11th video\n",
      "4 batches generated\n",
      "12th video\n",
      "4 batches generated\n",
      "13th video\n",
      "4 batches generated\n",
      "14th video\n",
      "3 batches generated\n",
      "15th video\n",
      "3 batches generated\n",
      "16th video\n",
      "3 batches generated\n",
      "17th video\n",
      "3 batches generated\n",
      "18th video\n",
      "1 batches generated\n",
      "19th video\n",
      "3 batches generated\n",
      "20th video\n",
      "2 batches generated\n",
      "21th video\n",
      "22th video\n",
      "23th video\n",
      "2 batches generated\n",
      "24th video\n",
      "4 batches generated\n",
      "25th video\n",
      "26th video\n",
      "27th video\n",
      "1 batches generated\n",
      "28th video\n",
      "29th video\n",
      "2 batches generated\n",
      "30th video\n",
      "4 batches generated\n",
      "31th video\n",
      "3 batches generated\n",
      "32th video\n",
      "2 batches generated\n",
      "33th video\n",
      "6 batches generated\n",
      "34th video\n",
      "4 batches generated\n",
      "35th video\n",
      "12 batches generated\n",
      "36th video\n",
      "12 batches generated\n",
      "37th video\n",
      "12 batches generated\n",
      "38th video\n",
      "3 batches generated\n",
      "39th video\n",
      "4 batches generated\n",
      "40th video\n",
      "1 batches generated\n",
      "41th video\n",
      "4 batches generated\n",
      "42th video\n",
      "1 batches generated\n",
      "43th video\n",
      "28 batches generated\n",
      "44th video\n",
      "50 batches generated\n",
      "45th video\n",
      "35 batches generated\n",
      "46th video\n",
      "56 batches generated\n",
      "47th video\n",
      "1 batches generated\n",
      "48th video\n",
      "49th video\n",
      "12 batches generated\n",
      "50th video\n",
      "1 batches generated\n",
      "51th video\n",
      "2 batches generated\n",
      "52th video\n",
      "7 batches generated\n",
      "53th video\n",
      "23 batches generated\n",
      "54th video\n",
      "14 batches generated\n",
      "55th video\n",
      "56th video\n",
      "4 batches generated\n",
      "57th video\n",
      "6 batches generated\n",
      "58th video\n",
      "59th video\n",
      "60th video\n",
      "61th video\n",
      "3 batches generated\n",
      "62th video\n",
      "2 batches generated\n",
      "63th video\n",
      "3 batches generated\n",
      "64th video\n",
      "5 batches generated\n",
      "65th video\n",
      "66th video\n",
      "3 batches generated\n",
      "====== 6:05:04.484600 ======\n",
      "------------All train finished---------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime    #second to hour\n",
    "\n",
    "t_steps = size_sample - 1\n",
    "\n",
    "# define model\n",
    "#input (timesteps, features)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(L.LSTM(100, activation='relu', batch_input_shape=(size_batch, t_steps, size_grid)))\n",
    "model.add(L.Dense(1)) #number of prediction\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "X = []\n",
    "X_list = []\n",
    "y = []\n",
    "y_list = []\n",
    "\n",
    "#train start\n",
    "startTime = time.time()\n",
    "\n",
    "#define dataset\n",
    "#batch size = all cases from videos\n",
    "for v_i in range(len(total_video)):\n",
    "\tprint(\"%dth video\"%int(v_i+1))\n",
    "\tif size_batch <= len(total_video[v_i]): \n",
    "\t\tn_batch = int(len(total_video[v_i])/size_batch)\n",
    "\t\tprint(\"%d batches generated\"%n_batch)\n",
    "                      \n",
    "\t\tfor i in range(n_batch):                      \n",
    "\t\t\tfor k in range(size_batch):                      \n",
    "\t\t\t\t# X   ex) 20 size_sample, 19 timesteps for X and last one for Y\n",
    "\t\t\t\tinputs = []\n",
    "\t\t\t\tfor j in range(t_steps):         \n",
    "\t\t\t\t\tinputs.append(total_video[v_i][i*size_batch + k][j]) #v_i th video, ith case, jth timestep                                       \n",
    "\t\t\t\tX_list.append(inputs)\n",
    "\t\t\t\t# y\n",
    "\t\t\t\ty_list.append(total_video[v_i][i*size_batch + k][t_steps]) #v_i th video, ith case          \n",
    "\t\t\tX = array(X_list)\n",
    "\t\t\ty = array(y_list)\n",
    "           \n",
    "\t\t\t#reshape\n",
    "\t\t\tX.reshape((size_batch, t_steps, size_grid))            \n",
    "\n",
    "\t\t\t# fit model\n",
    "\t\t\tmodel.fit(X, y, batch_size=size_batch, epochs=100, steps_per_epoch=size_sample, verbose=0)\n",
    "    \n",
    "\t\t\t#flush  \n",
    "\t\t\tX = [] \n",
    "\t\t\tX_list = []\n",
    "\t\t\ty = []\n",
    "\t\t\ty_list = []    \n",
    "    \n",
    "\n",
    "endTime = time.time() - startTime\n",
    "hourtime = str(datetime.timedelta(seconds = endTime))\n",
    "print(\"====== \" + hourtime + \" ======\")\n",
    "    \n",
    "    \n",
    "#and save(keras, with 'save', architecture and model are saved simultaneously)\n",
    "name = str('C:/Users/zoclz/models/2finalmodel2.h5')\n",
    "model.save(name)\n",
    "\n",
    "    \n",
    "print(\"------------All train finished---------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
